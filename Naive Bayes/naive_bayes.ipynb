{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from math import log\n",
    "from sklearn.metrics import mean_squared_error, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"messages/part\"\n",
    "SPAM = \"spam\"\n",
    "LEGIT = \"legit\"\n",
    "def getFiles(folderNumber):\n",
    "    curPath = path + str(folderNumber)\n",
    "    files = []\n",
    "    for filename in os.listdir(curPath):\n",
    "        file = open(curPath + \"/\" + filename, 'r')\n",
    "        subject = list(map(int, file.readline().split()[1:]))\n",
    "        file.readline()\n",
    "        text = list(map(int, file.readline().split()))\n",
    "        #print(filename)\n",
    "        if LEGIT in filename:\n",
    "            files.append((LEGIT, [subject, text]))\n",
    "        else:\n",
    "            files.append((SPAM, [subject, text]))\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(trains):\n",
    "    frequencies = defaultdict(lambda:0)\n",
    "    classes = defaultdict(lambda:0)\n",
    "    counts = defaultdict(lambda:0)\n",
    "    for spam_or_legit, file in trains:\n",
    "        for word in file[1]:\n",
    "            frequencies[spam_or_legit, word] += 1.0     #частота встречаемости слов из письма в данном классе\n",
    "            counts[word] += 1.0                         #количество данного слова в сообщении\n",
    "        classes[spam_or_legit] += len(file[1])          #количество слов у данного класса\n",
    "    for spam_or_legit, word in frequencies:\n",
    "        frequencies[spam_or_legit, word] = (frequencies[spam_or_legit, word] + alpha) / (counts[word] + alpha * len(counts))        # P(x_i | SPAM) || P(x_i | LEGIT)\n",
    "\n",
    "    for clazz in classes:\n",
    "        classes[clazz] /= len(trains)\n",
    "    return classes, frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingWithSubject(trains):\n",
    "    frequencies = defaultdict(lambda:0)\n",
    "    classes = defaultdict(lambda:0)\n",
    "    counts = defaultdict(lambda:0)\n",
    "    for spam_or_legit, file in trains:\n",
    "        for word in file[0]:\n",
    "            #frequencies[spam_or_legit, word] += (1.0 * (int((len(file[1]) / len(file[0])))))\n",
    "            frequencies[spam_or_legit, word] += 1.0\n",
    "            counts[word] += 1.0\n",
    "        for word in file[1]:\n",
    "            frequencies[spam_or_legit, word] += 1.0\n",
    "            counts[word] += 1.0\n",
    "        classes[spam_or_legit] += len(file[1]) + len(file[0])     # число слов такого-то класса\n",
    "    for spam_or_legit, word in frequencies:\n",
    "        frequencies[spam_or_legit, word] = (frequencies[spam_or_legit, word] + alpha) / (counts[word] + alpha * len(counts))        # P(x_i | C) || P(x_i | C)\n",
    "    for clazz in classes:\n",
    "        classes[clazz] /= len(trains)      # число слов такого-то класса / общее число слов\n",
    "    return classes, frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorWeight = defaultdict(lambda:1)\n",
    "errorWeight[SPAM] = 1\n",
    "errorWeight[LEGIT] = 1\n",
    "def classify(classifier, file):\n",
    "    classes, frequencies = classifier\n",
    "    # result class = argmax(C, P(C)*p(P(x_i|C))) = argmin(C, -log(P(C) - sum(log(P(x_i|C)))))\n",
    "    spamProb = -log(classes[SPAM])\n",
    "    legitProb = -log(classes[LEGIT])\n",
    "    for word in file[1]:\n",
    "        spamProb -= log(frequencies[SPAM, word] + 10**(-7))\n",
    "        legitProb -= log(frequencies[LEGIT, word] + 10**(-7))\n",
    "    spamProb -= log(errorWeight[SPAM])\n",
    "    legitProb -= log(errorWeight[LEGIT])\n",
    "    if (spamProb < legitProb):\n",
    "        return SPAM\n",
    "    return LEGIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda_legit=1\n",
      "mse =  0.027522935779816515\n",
      "accuracy =  0.9724770642201835\n",
      "f1 score =  0.9696969696969697\n",
      "test count =  109\n",
      "Спам в спам =  58\n",
      "Спам в реальные =  0\n",
      "Реальные в спам =  3\n",
      "Реальные в реальные =  48\n",
      "mse =  0.022935779816513763\n",
      "accuracy =  0.9770642201834863\n",
      "f1 score =  0.9746192893401014\n",
      "test count =  218\n",
      "Спам в спам =  117\n",
      "Спам в реальные =  0\n",
      "Реальные в спам =  5\n",
      "Реальные в реальные =  96\n",
      "mse =  0.01834862385321101\n",
      "accuracy =  0.981651376146789\n",
      "f1 score =  0.9795918367346939\n",
      "test count =  327\n",
      "Спам в спам =  177\n",
      "Спам в реальные =  0\n",
      "Реальные в спам =  6\n",
      "Реальные в реальные =  144\n",
      "mse =  0.013761467889908258\n",
      "accuracy =  0.9862385321100917\n",
      "f1 score =  0.9846153846153847\n",
      "test count =  436\n",
      "Спам в спам =  238\n",
      "Спам в реальные =  0\n",
      "Реальные в спам =  6\n",
      "Реальные в реальные =  192\n",
      "mse =  0.01834862385321101\n",
      "accuracy =  0.981651376146789\n",
      "f1 score =  0.9793388429752066\n",
      "test count =  545\n",
      "Спам в спам =  298\n",
      "Спам в реальные =  3\n",
      "Реальные в спам =  7\n",
      "Реальные в реальные =  237\n",
      "mse =  0.021406727828746176\n",
      "accuracy =  0.9785932721712538\n",
      "f1 score =  0.9757785467128027\n",
      "test count =  654\n",
      "Спам в спам =  358\n",
      "Спам в реальные =  6\n",
      "Реальные в спам =  8\n",
      "Реальные в реальные =  282\n",
      "mse =  0.020969855832241154\n",
      "accuracy =  0.9790301441677588\n",
      "f1 score =  0.9762611275964391\n",
      "test count =  763\n",
      "Спам в спам =  418\n",
      "Спам в реальные =  7\n",
      "Реальные в спам =  9\n",
      "Реальные в реальные =  329\n",
      "mse =  0.0194954128440367\n",
      "accuracy =  0.9805045871559633\n",
      "f1 score =  0.9779507133592736\n",
      "test count =  872\n",
      "Спам в спам =  478\n",
      "Спам в реальные =  7\n",
      "Реальные в спам =  10\n",
      "Реальные в реальные =  377\n",
      "mse =  0.021406727828746176\n",
      "accuracy =  0.9785932721712538\n",
      "f1 score =  0.975889781859931\n",
      "test count =  981\n",
      "Спам в спам =  535\n",
      "Спам в реальные =  7\n",
      "Реальные в спам =  14\n",
      "Реальные в реальные =  425\n",
      "mse =  0.02110091743119266\n",
      "accuracy =  0.9788990825688073\n",
      "f1 score =  0.9762150982419856\n",
      "test count =  1090\n",
      "Спам в спам =  595\n",
      "Спам в реальные =  8\n",
      "Реальные в спам =  15\n",
      "Реальные в реальные =  472\n",
      "Lambda_legit=10\n",
      "mse =  0.01834862385321101\n",
      "accuracy =  0.981651376146789\n",
      "f1 score =  0.9795918367346939\n",
      "test count =  109\n",
      "Спам в спам =  59\n",
      "Спам в реальные =  0\n",
      "Реальные в спам =  2\n",
      "Реальные в реальные =  48\n",
      "mse =  0.01834862385321101\n",
      "accuracy =  0.981651376146789\n",
      "f1 score =  0.9795918367346939\n",
      "test count =  218\n",
      "Спам в спам =  118\n",
      "Спам в реальные =  0\n",
      "Реальные в спам =  4\n",
      "Реальные в реальные =  96\n",
      "mse =  0.01529051987767584\n",
      "accuracy =  0.9847094801223242\n",
      "f1 score =  0.9829351535836178\n",
      "test count =  327\n",
      "Спам в спам =  178\n",
      "Спам в реальные =  0\n",
      "Реальные в спам =  5\n",
      "Реальные в реальные =  144\n",
      "mse =  0.013761467889908258\n",
      "accuracy =  0.9862385321100917\n",
      "f1 score =  0.9845360824742269\n",
      "test count =  436\n",
      "Спам в спам =  239\n",
      "Спам в реальные =  1\n",
      "Реальные в спам =  5\n",
      "Реальные в реальные =  191\n",
      "mse =  0.01834862385321101\n",
      "accuracy =  0.981651376146789\n",
      "f1 score =  0.979253112033195\n",
      "test count =  545\n",
      "Спам в спам =  299\n",
      "Спам в реальные =  4\n",
      "Реальные в спам =  6\n",
      "Реальные в реальные =  236\n",
      "mse =  0.021406727828746176\n",
      "accuracy =  0.9785932721712538\n",
      "f1 score =  0.9756944444444444\n",
      "test count =  654\n",
      "Спам в спам =  359\n",
      "Спам в реальные =  7\n",
      "Реальные в спам =  7\n",
      "Реальные в реальные =  281\n",
      "mse =  0.020969855832241154\n",
      "accuracy =  0.9790301441677588\n",
      "f1 score =  0.9761904761904762\n",
      "test count =  763\n",
      "Спам в спам =  419\n",
      "Спам в реальные =  8\n",
      "Реальные в спам =  8\n",
      "Реальные в реальные =  328\n",
      "mse =  0.0194954128440367\n",
      "accuracy =  0.9805045871559633\n",
      "f1 score =  0.9778933680104032\n",
      "test count =  872\n",
      "Спам в спам =  479\n",
      "Спам в реальные =  8\n",
      "Реальные в спам =  9\n",
      "Реальные в реальные =  376\n",
      "mse =  0.021406727828746176\n",
      "accuracy =  0.9785932721712538\n",
      "f1 score =  0.9758342922899885\n",
      "test count =  981\n",
      "Спам в спам =  536\n",
      "Спам в реальные =  8\n",
      "Реальные в спам =  13\n",
      "Реальные в реальные =  424\n",
      "mse =  0.02110091743119266\n",
      "accuracy =  0.9788990825688073\n",
      "f1 score =  0.9761658031088083\n",
      "test count =  1090\n",
      "Спам в спам =  596\n",
      "Спам в реальные =  9\n",
      "Реальные в спам =  14\n",
      "Реальные в реальные =  471\n",
      "Lambda_legit=100\n",
      "mse =  0.01834862385321101\n",
      "accuracy =  0.981651376146789\n",
      "f1 score =  0.9795918367346939\n",
      "test count =  109\n",
      "Спам в спам =  59\n",
      "Спам в реальные =  0\n",
      "Реальные в спам =  2\n",
      "Реальные в реальные =  48\n",
      "mse =  0.013761467889908258\n",
      "accuracy =  0.9862385321100917\n",
      "f1 score =  0.9846153846153847\n",
      "test count =  218\n",
      "Спам в спам =  119\n",
      "Спам в реальные =  0\n",
      "Реальные в спам =  3\n",
      "Реальные в реальные =  96\n",
      "mse =  0.01529051987767584\n",
      "accuracy =  0.9847094801223242\n",
      "f1 score =  0.9828178694158075\n",
      "test count =  327\n",
      "Спам в спам =  179\n",
      "Спам в реальные =  1\n",
      "Реальные в спам =  4\n",
      "Реальные в реальные =  143\n",
      "mse =  0.013761467889908258\n",
      "accuracy =  0.9862385321100917\n",
      "f1 score =  0.9844559585492227\n",
      "test count =  436\n",
      "Спам в спам =  240\n",
      "Спам в реальные =  2\n",
      "Реальные в спам =  4\n",
      "Реальные в реальные =  190\n",
      "mse =  0.01834862385321101\n",
      "accuracy =  0.981651376146789\n",
      "f1 score =  0.9791666666666666\n",
      "test count =  545\n",
      "Спам в спам =  300\n",
      "Спам в реальные =  5\n",
      "Реальные в спам =  5\n",
      "Реальные в реальные =  235\n",
      "mse =  0.021406727828746176\n",
      "accuracy =  0.9785932721712538\n",
      "f1 score =  0.975609756097561\n",
      "test count =  654\n",
      "Спам в спам =  360\n",
      "Спам в реальные =  8\n",
      "Реальные в спам =  6\n",
      "Реальные в реальные =  280\n",
      "mse =  0.019659239842726082\n",
      "accuracy =  0.9803407601572739\n",
      "f1 score =  0.9775784753363228\n",
      "test count =  763\n",
      "Спам в спам =  421\n",
      "Спам в реальные =  9\n",
      "Реальные в спам =  6\n",
      "Реальные в реальные =  327\n",
      "mse =  0.01834862385321101\n",
      "accuracy =  0.981651376146789\n",
      "f1 score =  0.9791122715404701\n",
      "test count =  872\n",
      "Спам в спам =  481\n",
      "Спам в реальные =  9\n",
      "Реальные в спам =  7\n",
      "Реальные в реальные =  375\n",
      "mse =  0.020387359836901122\n",
      "accuracy =  0.9796126401630989\n",
      "f1 score =  0.9769053117782909\n",
      "test count =  981\n",
      "Спам в спам =  538\n",
      "Спам в реальные =  9\n",
      "Реальные в спам =  11\n",
      "Реальные в реальные =  423\n",
      "mse =  0.02018348623853211\n",
      "accuracy =  0.9798165137614679\n",
      "f1 score =  0.9771309771309771\n",
      "test count =  1090\n",
      "Спам в спам =  598\n",
      "Спам в реальные =  10\n",
      "Реальные в спам =  12\n",
      "Реальные в реальные =  470\n",
      "Lambda_legit=1000\n",
      "mse =  0.01834862385321101\n",
      "accuracy =  0.981651376146789\n",
      "f1 score =  0.9795918367346939\n",
      "test count =  109\n",
      "Спам в спам =  59\n",
      "Спам в реальные =  0\n",
      "Реальные в спам =  2\n",
      "Реальные в реальные =  48\n",
      "mse =  0.013761467889908258\n",
      "accuracy =  0.9862385321100917\n",
      "f1 score =  0.9846153846153847\n",
      "test count =  218\n",
      "Спам в спам =  119\n",
      "Спам в реальные =  0\n",
      "Реальные в спам =  3\n",
      "Реальные в реальные =  96\n",
      "mse =  0.01529051987767584\n",
      "accuracy =  0.9847094801223242\n",
      "f1 score =  0.9828178694158075\n",
      "test count =  327\n",
      "Спам в спам =  179\n",
      "Спам в реальные =  1\n",
      "Реальные в спам =  4\n",
      "Реальные в реальные =  143\n",
      "mse =  0.013761467889908258\n",
      "accuracy =  0.9862385321100917\n",
      "f1 score =  0.9844559585492227\n",
      "test count =  436\n",
      "Спам в спам =  240\n",
      "Спам в реальные =  2\n",
      "Реальные в спам =  4\n",
      "Реальные в реальные =  190\n",
      "mse =  0.01834862385321101\n",
      "accuracy =  0.981651376146789\n",
      "f1 score =  0.9791666666666666\n",
      "test count =  545\n",
      "Спам в спам =  300\n",
      "Спам в реальные =  5\n",
      "Реальные в спам =  5\n",
      "Реальные в реальные =  235\n",
      "mse =  0.021406727828746176\n",
      "accuracy =  0.9785932721712538\n",
      "f1 score =  0.975609756097561\n",
      "test count =  654\n",
      "Спам в спам =  360\n",
      "Спам в реальные =  8\n",
      "Реальные в спам =  6\n",
      "Реальные в реальные =  280\n",
      "mse =  0.019659239842726082\n",
      "accuracy =  0.9803407601572739\n",
      "f1 score =  0.9775784753363228\n",
      "test count =  763\n",
      "Спам в спам =  421\n",
      "Спам в реальные =  9\n",
      "Реальные в спам =  6\n",
      "Реальные в реальные =  327\n",
      "mse =  0.01834862385321101\n",
      "accuracy =  0.981651376146789\n",
      "f1 score =  0.9791122715404701\n",
      "test count =  872\n",
      "Спам в спам =  481\n",
      "Спам в реальные =  9\n",
      "Реальные в спам =  7\n",
      "Реальные в реальные =  375\n",
      "mse =  0.020387359836901122\n",
      "accuracy =  0.9796126401630989\n",
      "f1 score =  0.9769053117782909\n",
      "test count =  981\n",
      "Спам в спам =  538\n",
      "Спам в реальные =  9\n",
      "Реальные в спам =  11\n",
      "Реальные в реальные =  423\n",
      "mse =  0.02018348623853211\n",
      "accuracy =  0.9798165137614679\n",
      "f1 score =  0.9771309771309771\n",
      "test count =  1090\n",
      "Спам в спам =  598\n",
      "Спам в реальные =  10\n",
      "Реальные в спам =  12\n",
      "Реальные в реальные =  470\n",
      "Lambda_legit=10000\n",
      "mse =  0.027522935779816515\n",
      "accuracy =  0.9724770642201835\n",
      "f1 score =  0.9690721649484536\n",
      "test count =  109\n",
      "Спам в спам =  59\n",
      "Спам в реальные =  1\n",
      "Реальные в спам =  2\n",
      "Реальные в реальные =  47\n",
      "mse =  0.01834862385321101\n",
      "accuracy =  0.981651376146789\n",
      "f1 score =  0.9793814432989691\n",
      "test count =  218\n",
      "Спам в спам =  119\n",
      "Спам в реальные =  1\n",
      "Реальные в спам =  3\n",
      "Реальные в реальные =  95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse =  0.024464831804281346\n",
      "accuracy =  0.9755351681957186\n",
      "f1 score =  0.9722222222222222\n",
      "test count =  327\n",
      "Спам в спам =  179\n",
      "Спам в реальные =  4\n",
      "Реальные в спам =  4\n",
      "Реальные в реальные =  140\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-217-80d336f04afe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrains\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrains\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mmeanClassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainingWithSubject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mclazz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mpredClass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeanClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-199-f70ca48115a1>\u001b[0m in \u001b[0;36mtrainingWithSubject\u001b[0;34m(trains)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mfrequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspam_or_legit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspam_or_legit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# число документов такого-то класса\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "folds = [getFiles(i) for i in range(1, 11)]\n",
    "ar = []\n",
    "for f in folds:\n",
    "    ar = ar + f\n",
    "\n",
    "mean_score = []\n",
    "eWeights = []\n",
    "errorWeight[SPAM] = 1\n",
    "errorWeight[LEGIT] = 1\n",
    "best_alpha = 0.0\n",
    "best_score = 0.0\n",
    "#best_lambda = 10**5\n",
    "xs = []\n",
    "alpha = 0.01\n",
    "i = 0\n",
    "while errorWeight[LEGIT] < 10**10:\n",
    "    print(\"Lambda_legit=\" + str(errorWeight[LEGIT]))\n",
    "    predict = []\n",
    "    answer = []\n",
    "    counts = [[0, 0], [0, 0]]\n",
    "    cnt = 0\n",
    "\n",
    "#while alpha <= 1.0:\n",
    "    eWeights.append(errorWeight[LEGIT])\n",
    "    sum_score = 0.0\n",
    "    for test in folds:\n",
    "        #print(\"test \" + str(i))\n",
    "        meanClassifier = defaultdict(lambda:0), defaultdict(lambda:0)\n",
    "        ar = []\n",
    "        for trains in folds:\n",
    "            if trains != test:\n",
    "                ar = ar + trains\n",
    "            meanClassifier = trainingWithSubject(ar)\n",
    "        for clazz, file in test:\n",
    "            predClass = classify(meanClassifier, file)\n",
    "            predict.append(int(predClass == SPAM))\n",
    "            answer.append(int(clazz == SPAM))\n",
    "            counts[predClass == SPAM][clazz == SPAM] += 1\n",
    "            cnt += 1\n",
    "        sum_score += (accuracy_score(predict, answer))\n",
    "        #i += 1\n",
    "        print(\"mse = \", mean_squared_error(predict, answer))\n",
    "        print(\"accuracy = \", accuracy_score(predict, answer))\n",
    "        print(\"f1 score = \", f1_score(predict, answer))\n",
    "        print(\"test count = \", cnt)\n",
    "        print(\"Спам в спам = \", counts[0][0])\n",
    "        print(\"Спам в реальные = \", counts[0][1])\n",
    "        print(\"Реальные в спам = \", counts[1][0])\n",
    "        print(\"Реальные в реальные = \", counts[1][1])\n",
    "    i += 1\n",
    "    xs.append(i)\n",
    "    mean_score.append(sum_score / 10.0)\n",
    "    errorWeight[LEGIT] *= 10\n",
    "#if sum_score > best_score:\n",
    "    #best_score = sum_score\n",
    "    #best_alpha = alpha\n",
    "    #best_lambda = errorWeight[SPAM]\n",
    "#alpha += 0.01\n",
    "\n",
    "#errorWeight[SPAM] *= (10)\n",
    "\n",
    "#print(best_score)\n",
    "print(mean_score)\n",
    "#print(best_lambda)\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(eWeights, mean_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eWeights, mean_score)\n",
    "plt.figure(figsize=(15, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse =  0.22018348623853212\n",
      "accuracy =  0.7798165137614679\n",
      "f1 score =  0.6666666666666666\n",
      "test count =  109\n",
      "Спам в спам =  61\n",
      "Спам в реальные =  24\n",
      "Реальные в спам =  0\n",
      "Реальные в реальные =  24\n",
      "mse =  0.22935779816513763\n",
      "accuracy =  0.7706422018348624\n",
      "f1 score =  0.647887323943662\n",
      "test count =  218\n",
      "Спам в спам =  122\n",
      "Спам в реальные =  50\n",
      "Реальные в спам =  0\n",
      "Реальные в реальные =  46\n",
      "mse =  0.23547400611620795\n",
      "accuracy =  0.764525993883792\n",
      "f1 score =  0.6350710900473934\n",
      "test count =  327\n",
      "Спам в спам =  183\n",
      "Спам в реальные =  77\n",
      "Реальные в спам =  0\n",
      "Реальные в реальные =  67\n",
      "mse =  0.22935779816513763\n",
      "accuracy =  0.7706422018348624\n",
      "f1 score =  0.647887323943662\n",
      "test count =  436\n",
      "Спам в спам =  244\n",
      "Спам в реальные =  100\n",
      "Реальные в спам =  0\n",
      "Реальные в реальные =  92\n",
      "mse =  0.23302752293577983\n",
      "accuracy =  0.7669724770642202\n",
      "f1 score =  0.6402266288951842\n",
      "test count =  545\n",
      "Спам в спам =  305\n",
      "Спам в реальные =  127\n",
      "Реальные в спам =  0\n",
      "Реальные в реальные =  113\n",
      "mse =  0.23700305810397554\n",
      "accuracy =  0.7629969418960245\n",
      "f1 score =  0.6318289786223278\n",
      "test count =  654\n",
      "Спам в спам =  366\n",
      "Спам в реальные =  155\n",
      "Реальные в спам =  0\n",
      "Реальные в реальные =  133\n",
      "mse =  0.2306684141546527\n",
      "accuracy =  0.7693315858453473\n",
      "f1 score =  0.6451612903225806\n",
      "test count =  763\n",
      "Спам в спам =  427\n",
      "Спам в реальные =  176\n",
      "Реальные в спам =  0\n",
      "Реальные в реальные =  160\n",
      "mse =  0.2305045871559633\n",
      "accuracy =  0.7694954128440367\n",
      "f1 score =  0.6455026455026455\n",
      "test count =  872\n",
      "Спам в спам =  488\n",
      "Спам в реальные =  201\n",
      "Реальные в спам =  0\n",
      "Реальные в реальные =  183\n",
      "mse =  0.22935779816513763\n",
      "accuracy =  0.7706422018348624\n",
      "f1 score =  0.647887323943662\n",
      "test count =  981\n",
      "Спам в спам =  549\n",
      "Спам в реальные =  225\n",
      "Реальные в спам =  0\n",
      "Реальные в реальные =  207\n",
      "mse =  0.23577981651376148\n",
      "accuracy =  0.7642201834862385\n",
      "f1 score =  0.6344238975817923\n",
      "test count =  1090\n",
      "Спам в спам =  610\n",
      "Спам в реальные =  257\n",
      "Реальные в спам =  0\n",
      "Реальные в реальные =  223\n"
     ]
    }
   ],
   "source": [
    "predict = []\n",
    "answer = []\n",
    "counts = [[0, 0], [0, 0]]\n",
    "cnt = 0\n",
    "errorWeight[LEGIT] = 10**70\n",
    "alpha = 0.01\n",
    "sum_score = 0.0\n",
    "for test in folds:\n",
    "    meanClassifier = defaultdict(lambda:0), defaultdict(lambda:0)\n",
    "    ar = []\n",
    "    for trains in folds:\n",
    "        if trains != test:\n",
    "            ar = ar + trains\n",
    "        meanClassifier = trainingWithSubject(ar)\n",
    "    for clazz, file in test:\n",
    "        predClass = classify(meanClassifier, file)\n",
    "        predict.append(int(predClass == SPAM))\n",
    "        answer.append(int(clazz == SPAM))\n",
    "        counts[predClass == SPAM][clazz == SPAM] += 1\n",
    "        cnt += 1\n",
    "    sum_score += (accuracy_score(predict, answer))\n",
    "    print(\"mse = \", mean_squared_error(predict, answer))\n",
    "    print(\"accuracy = \", accuracy_score(predict, answer))\n",
    "    print(\"f1 score = \", f1_score(predict, answer))\n",
    "    print(\"test count = \", cnt)\n",
    "    print(\"Спам в спам = \", counts[0][0])\n",
    "    print(\"Спам в реальные = \", counts[0][1])\n",
    "    print(\"Реальные в спам = \", counts[1][0])\n",
    "    print(\"Реальные в реальные = \", counts[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
